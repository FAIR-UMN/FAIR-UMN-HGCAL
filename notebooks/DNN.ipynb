{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from models.DNN import NN\n",
    "from dataset_utils import H5DatasetDNN, split_dataset\n",
    "from utils.torch_utils import MARELoss, train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc5cee",
   "metadata": {},
   "source": [
    "### Define the model parameters:\n",
    "-  **EPOCHS**: Number of epochs to be used for training the model\n",
    "-  **TRAIN_BATCH_SIZE**: Batch size\n",
    "-  **LAYERS**: A list which includes the number of nodes in each layer of the DNN starting with the input layer and ending with the output layer\n",
    "-  **LRATE**: Learning rate\n",
    "-  **TRAINING_FOLDER**: Folder to store the model summary and weights after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "LAYERS = [28, 40, 35, 1] # shape of DNN\n",
    "LRATE = 1e-3 # learning rate\n",
    "TRAINING_FOLDER=\"../training/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac322b4",
   "metadata": {},
   "source": [
    "### Loading Dataset:\n",
    "Use the custom H5DatasetDNN dataloader and split the dataset into two categories for training and testing. The dataset can be also converted to the pickle format and one can use PklDatasetDNN class instead. One can also write a custom function and add it dataset_utils.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8851e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/rusack/shared/hdf5/hgcal_electron//hgcal_electron_data_test.h5'\n",
    "dataset = H5DatasetDNN(file_path)\n",
    "train_test_datasets = split_dataset(dataset)\n",
    "\n",
    "X = train_test_datasets['train']\n",
    "Y = train_test_datasets['test']\n",
    "\n",
    "dataloaders = { 'train': torch.utils.data.DataLoader(X, TRAIN_BATCH_SIZE, shuffle=True),\n",
    "                'test': torch.utils.data.DataLoader(Y, len(Y), shuffle=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58dd9e1",
   "metadata": {},
   "source": [
    "Run the training and add losses and learning rates to a list which can then be saved as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = tqdm(range(EPOCHS))\n",
    "for epoch in pbar:\n",
    "    for xtrain, ytrain in dataloaders['train']:\n",
    "        train_loss, output_train = train(nn, xtrain, ytrain, optimizer, loss_func)\n",
    "        \n",
    "        test_loss = None\n",
    "        output_test = None\n",
    "        with torch.no_grad():\n",
    "            for xtest, ytest in dataloaders['test']:\n",
    "                xtest = torch.reshape(nn(xtest), (-1,))\n",
    "                test_loss = MARELoss(xtest, ytest)\n",
    "        \n",
    "        epochs.append(epoch)\n",
    "        train_loss_array.append(train_loss.item())\n",
    "        valid_loss_array.append(test_loss.item())\n",
    "        lr_array.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        pbar.set_postfix({'training loss': train_loss.item(), 'validation loss': test_loss.item()})\n",
    "        torch.save(nn.state_dict(), f'{TRAINING_FOLDER}/epoch{epoch}')\n",
    "\n",
    "training_summary = {\n",
    "    'epochs': epochs,\n",
    "    'train_loss': train_loss_array,\n",
    "    'valid_loss': valid_loss_array,\n",
    "    'learning_rate': lr_array\n",
    "}\n",
    "\n",
    "with open(f'{TRAINING_FOLDER}/summary.pkl','wb') as f_:\n",
    "    pickle.dump(training_summary, f_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1644e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27daad2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df2a6374",
   "metadata": {},
   "source": [
    " lr_array\n",
    "}\n",
    "\n",
    "with open(f'{TRAINING_FOLDER}/summary.pkl','wb') as f_:\n",
    "    pickle.dump(training_summary, f_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.9",
   "language": "python",
   "name": "torch1.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
